{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# Load data\n",
    "book_names = np.loadtxt('Books.csv', skiprows=1, delimiter=';', usecols=(0,), dtype=str)\n",
    "userid, bookname, rating = np.loadtxt(\n",
    "    'Ratings.csv',\n",
    "    skiprows=1,\n",
    "    delimiter=';',\n",
    "    dtype=str,\n",
    "    comments=None,\n",
    "    unpack=True\n",
    ")\n",
    "\n",
    "# Get unique users and books\n",
    "unique_users = np.unique(userid)\n",
    "book_to_index = {book: idx for idx, book in enumerate(book_names)}\n",
    "user_to_index = {user: idx for idx, user in enumerate(unique_users)}\n",
    "\n",
    "# Create a sparse rating matrix\n",
    "rating_matrix = lil_matrix((len(unique_users), len(book_names)), dtype=np.float32)\n",
    "\n",
    "# Populate the sparse matrix\n",
    "for user, book, rate in zip(userid, bookname, rating):\n",
    "    user_idx = user_to_index[user]\n",
    "    book_idx = book_to_index.get(book, None)\n",
    "    if book_idx is not None:  # Skip books not found in book_names\n",
    "        rating_matrix[user_idx, book_idx] = float(rate)\n",
    "\n",
    "# You can convert the matrix to other formats (e.g., CSR) if needed\n",
    "rating_matrix = rating_matrix.tocsr()\n",
    "#reduce dimenality \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "rating_matrix = svd.fit_transform(rating_matrix)\n",
    "#plot components\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(svd.explained_variance_ratio_)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useing only 20 components\n",
    "svd = TruncatedSVD(n_components=5)\n",
    "rating_matrix = svd.fit_transform(rating_matrix)\n",
    "#deallocation\n",
    "del book_names, userid, bookname, rating, unique_users, book_to_index, user_to_index, user_idx, book_idx, rate, user, book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Define the forward pass for the neural network\n",
    "def forward_pass(Z, weights, activations):\n",
    "    for W, phi in zip(weights, activations):\n",
    "        Z = phi(W @ Z) \n",
    "    return Z\n",
    "def compute_loss(Y, Y_pred, S):\n",
    "    loss = 0\n",
    "    for i, j in S:\n",
    "        loss += (Y[i, j] - Y_pred[i, j]) ** 2\n",
    "    return loss\n",
    "# Compute gradients using backpropagation\n",
    "def backprop(Z, Y, Y_pred, S, weights, activations):\n",
    "    grads = [np.zeros_like(W) for W in weights]\n",
    "    m, n = Y.shape\n",
    "\n",
    "    # Gradient of the loss wrt Y_pred\n",
    "    dL_dY_pred = np.zeros((m, n))\n",
    "    for i, j in S:\n",
    "        dL_dY_pred[i, j] = -2 * (Y[i, j] - Y_pred[i, j])\n",
    "    dZ = dL_dY_pred\n",
    "    for l in reversed(range(len(weights))):\n",
    "        W = weights[l]\n",
    "        phi = activations[l]\n",
    "        Z_prev = forward_pass(Z, weights[:l], activations[:l])\n",
    "\n",
    "        # Gradient wrt weights\n",
    "#        grads[l] += dZ @ Z_prev.T\n",
    "        grads[l] += np.dot(dZ, Z_prev.T)  # Matrix multiplication\n",
    "\n",
    "        # Gradient wrt Z_prev\n",
    "        dZ = W.T @ dZ * (Z_prev > 0)  # Derivative of ReLU\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def gradient_descent(weights, grads, lr):\n",
    "    for l in range(len(weights)):\n",
    "        weights[l] -= lr * grads[l]\n",
    "\n",
    "def initialize_weights(layers):\n",
    "    weights = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        W = np.random.randn(layers[i + 1], layers[i]) * 0.01  # Small random values\n",
    "        weights.append(W)\n",
    "    return weights\n",
    "\n",
    "# Main function\n",
    "def matrix_completion_sgd(Y, S, layers, lr=0.01, epochs=50000, batch_size=64):\n",
    "    m, n = Y.shape\n",
    "    p = layers[0]\n",
    "\n",
    "    # Initialize input embedding and weights\n",
    "    Z = np.random.randn(p, n)  # Random input embedding\n",
    "    weights = initialize_weights(layers)\n",
    "    activations = [relu] * (len(layers) - 1)  # ReLU for all hidden layers\n",
    "\n",
    "    # Get observed entries\n",
    "    observed_indices = np.argwhere(S == 1)\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle observed indices\n",
    "        np.random.shuffle(observed_indices)\n",
    "        for i in range(0, len(observed_indices), batch_size):\n",
    "            # Mini-batch indices\n",
    "            batch_indices = observed_indices[i : i + batch_size]\n",
    "            # Extract mini-batch data\n",
    "            batch_Y = np.zeros_like(Y)\n",
    "            batch_S = np.zeros_like(S)\n",
    "            for idx in batch_indices:\n",
    "                row, col = idx\n",
    "                batch_Y[row, col] = Y[row, col]\n",
    "                batch_S[row, col] = 1\n",
    "\n",
    "            # Forward pass\n",
    "            Y_pred = forward_pass(Z, weights, activations)\n",
    "\n",
    "            # Compute loss for the mini-batch\n",
    "            loss = compute_loss(batch_Y, Y_pred, batch_S)\n",
    "\n",
    "            # Backpropagation for the mini-batch\n",
    "            grads = backprop(Z, batch_Y, Y_pred, batch_S, weights, activations)\n",
    "            # Gradient descent step\n",
    "            gradient_descent(weights, grads, lr)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed matrix Y: (200, 10)\n",
      "Sparsity mask S: (200, 10)\n",
      "Ground-truth matrix Y_true: (200, 10)\n",
      "Number of observed entries: 1817\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_synthetic_data(m, n, k, sparsity=0.7, noise_std=0.01):\n",
    "    \"\"\"\n",
    "        m (int): Number of users (rows in the matrix).\n",
    "        n (int): Number of items (columns in the matrix).\n",
    "        k (int): Number of hidden preference factors (rank of the matrix).\n",
    "        sparsity (float): Fraction of missing entries (0 = no missing data, 1 = fully sparse).\n",
    "        noise_std (float): Standard deviation of Gaussian noise added to the observations.\n",
    "        Y (ndarray): Observed matrix with missing entries.\n",
    "        S (ndarray): Binary mask indicating observed entries (1 for observed, 0 for missing).\n",
    "        Y_true (ndarray): Ground-truth low-rank matrix.\n",
    "    \"\"\"\n",
    "    user_preferences = np.random.randn(m, k)  # User \n",
    "    item_preferences = np.random.randn(k, n)  # features\n",
    "    Y_true = np.dot(user_preferences, item_preferences)  # Low-rank matrix (m x n)\n",
    "    Y_noisy = Y_true + noise_std * np.random.randn(m, n)\n",
    "    S = np.random.rand(m, n) > sparsity  # Sparsity mask (1 = observed, 0 = missing)\n",
    "    Y = np.where(S, Y_noisy, 0)  # Observed entries remain; missing entries set to 0\n",
    "\n",
    "    return Y, S, Y_true\n",
    "# Parameters\n",
    "m = 200           \n",
    "n = 10           \n",
    "rank = 4       \n",
    "sparsity = 0.1    # % entries missing\n",
    "noise_std = 0.0 #  Gaussian noise\n",
    "Y, S, Y_true = generate_synthetic_data(m, n, rank, sparsity, noise_std)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Observed matrix Y:\", Y.shape)\n",
    "print(\"Sparsity mask S:\", S.shape)\n",
    "print(\"Ground-truth matrix Y_true:\", Y_true.shape)\n",
    "print(\"Number of observed entries:\", np.sum(S))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 6948.176223198103\n",
      "Epoch 100, Loss: 6948.14931193827\n",
      "Epoch 200, Loss: 6948.122690152255\n",
      "Epoch 300, Loss: 6948.096249305134\n",
      "Epoch 400, Loss: 6948.070009653674\n",
      "Epoch 500, Loss: 6948.042770602307\n",
      "Epoch 600, Loss: 6948.015133104781\n",
      "Epoch 700, Loss: 6947.985724047231\n",
      "Epoch 800, Loss: 6947.955214001025\n",
      "Epoch 900, Loss: 6947.923313464234\n",
      "Epoch 1000, Loss: 6947.890780086463\n",
      "Epoch 1100, Loss: 6947.857787212285\n",
      "Epoch 1200, Loss: 6947.823342927888\n",
      "Epoch 1300, Loss: 6947.787492861689\n",
      "Epoch 1400, Loss: 6947.750292316724\n",
      "Epoch 1500, Loss: 6947.711715835777\n",
      "Epoch 1600, Loss: 6947.67146537224\n",
      "Epoch 1700, Loss: 6947.629017895323\n",
      "Epoch 1800, Loss: 6947.584628565883\n",
      "Epoch 1900, Loss: 6947.539344763941\n",
      "Epoch 2000, Loss: 6947.491357824223\n",
      "Epoch 2100, Loss: 6947.439794337104\n",
      "Epoch 2200, Loss: 6947.384869069005\n",
      "Epoch 2300, Loss: 6947.326797488988\n",
      "Epoch 2400, Loss: 6947.264374594028\n",
      "Epoch 2500, Loss: 6947.199124083188\n",
      "Epoch 2600, Loss: 6947.128984271747\n",
      "Epoch 2700, Loss: 6947.0533662272555\n",
      "Epoch 2800, Loss: 6946.97233666967\n",
      "Epoch 2900, Loss: 6946.886009399166\n",
      "Epoch 3000, Loss: 6946.792129591366\n",
      "Epoch 3100, Loss: 6946.691337501839\n",
      "Epoch 3200, Loss: 6946.583434477429\n",
      "Epoch 3300, Loss: 6946.466361231229\n",
      "Epoch 3400, Loss: 6946.3405479316925\n",
      "Epoch 3500, Loss: 6946.204798319437\n",
      "Epoch 3600, Loss: 6946.058597832718\n",
      "Epoch 3700, Loss: 6945.901789158883\n",
      "Epoch 3800, Loss: 6945.731213898257\n",
      "Epoch 3900, Loss: 6945.547357490427\n",
      "Epoch 4000, Loss: 6945.3490359881025\n",
      "Epoch 4100, Loss: 6945.1354968824\n",
      "Epoch 4200, Loss: 6944.906333769666\n",
      "Epoch 4300, Loss: 6944.659556928831\n",
      "Epoch 4400, Loss: 6944.392004005912\n",
      "Epoch 4500, Loss: 6944.103299984121\n",
      "Epoch 4600, Loss: 6943.7903363349715\n",
      "Epoch 4700, Loss: 6943.450571744359\n",
      "Epoch 4800, Loss: 6943.081499298629\n",
      "Epoch 4900, Loss: 6942.682704303562\n",
      "Epoch 5000, Loss: 6942.25094159594\n",
      "Epoch 5100, Loss: 6941.785830140459\n",
      "Epoch 5200, Loss: 6941.283508088836\n",
      "Epoch 5300, Loss: 6940.740320472185\n",
      "Epoch 5400, Loss: 6940.152004237386\n",
      "Epoch 5500, Loss: 6939.515355682288\n",
      "Epoch 5600, Loss: 6938.827441240867\n",
      "Epoch 5700, Loss: 6938.0838240261855\n",
      "Epoch 5800, Loss: 6937.280496673622\n",
      "Epoch 5900, Loss: 6936.416719441752\n",
      "Epoch 6000, Loss: 6935.484061820769\n",
      "Epoch 6100, Loss: 6934.472864746366\n",
      "Epoch 6200, Loss: 6933.379602318558\n",
      "Epoch 6300, Loss: 6932.1984485156645\n",
      "Epoch 6400, Loss: 6930.922320272692\n",
      "Epoch 6500, Loss: 6929.541372648662\n",
      "Epoch 6600, Loss: 6928.046006746972\n",
      "Epoch 6700, Loss: 6926.430996772881\n",
      "Epoch 6800, Loss: 6924.683820613323\n",
      "Epoch 6900, Loss: 6922.788836373004\n",
      "Epoch 7000, Loss: 6920.740218172326\n",
      "Epoch 7100, Loss: 6918.527616880342\n",
      "Epoch 7200, Loss: 6916.136244358351\n",
      "Epoch 7300, Loss: 6913.558513873268\n",
      "Epoch 7400, Loss: 6910.778370643411\n",
      "Epoch 7500, Loss: 6907.777408924361\n",
      "Epoch 7600, Loss: 6904.539432087035\n",
      "Epoch 7700, Loss: 6901.049725136142\n",
      "Epoch 7800, Loss: 6897.288336496325\n",
      "Epoch 7900, Loss: 6893.231080724149\n",
      "Epoch 8000, Loss: 6888.857761388757\n",
      "Epoch 8100, Loss: 6884.148860216143\n",
      "Epoch 8200, Loss: 6879.079213043539\n",
      "Epoch 8300, Loss: 6873.621012902453\n",
      "Epoch 8400, Loss: 6867.746583163347\n",
      "Epoch 8500, Loss: 6861.42688180689\n",
      "Epoch 8600, Loss: 6854.634499084968\n",
      "Epoch 8700, Loss: 6847.334472784622\n",
      "Epoch 8800, Loss: 6839.494471542158\n",
      "Epoch 8900, Loss: 6831.081241663435\n",
      "Epoch 9000, Loss: 6822.059625222539\n",
      "Epoch 9100, Loss: 6812.389447024143\n",
      "Epoch 9200, Loss: 6802.03327448201\n",
      "Epoch 9300, Loss: 6790.957647380965\n",
      "Epoch 9400, Loss: 6779.119080367223\n",
      "Epoch 9500, Loss: 6766.478282062083\n",
      "Epoch 9600, Loss: 6752.996497732195\n",
      "Epoch 9700, Loss: 6738.634948549177\n",
      "Epoch 9800, Loss: 6723.354048520756\n",
      "Epoch 9900, Loss: 6707.116511552367\n",
      "Completed Matrix:\n",
      " [[2.79549131e-04 0.00000000e+00 1.10654939e-02 ... 6.62462602e-04\n",
      "  1.17399738e-02 2.96885551e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.38756760e-02 0.00000000e+00 2.81525556e-02 ... 0.00000000e+00\n",
      "  4.81940345e-02 1.45772836e-01]\n",
      " ...\n",
      " [3.92632703e-03 2.19955630e-04 8.39457288e-02 ... 0.00000000e+00\n",
      "  9.38175724e-02 2.82819935e-01]\n",
      " [0.00000000e+00 0.00000000e+00 8.05947959e-04 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "layers = [n, 2 * n, m]\n",
    "# Create the sparsity mask and convert to list of indices\n",
    "S = list(zip(*np.where(S == 1)))  # Indices of observed values\n",
    "Y_pred = matrix_completion(Y, S, layers, lr=0.000001, epochs=10000)\n",
    "print(\"Completed Matrix:\\n\", Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-truth Matrix:\n",
      " [-0.68470024 -0.19471641  1.30766588  0.95764159  0.80160646 -0.70298037\n",
      "  0.91177458  0.12749228  0.28065081  0.26736104]\n",
      "Completed Matrix:\n",
      " [0.00027955 0.         0.01106549 0.02789739 0.01633329 0.01423968\n",
      " 0.         0.00066246 0.01173997 0.02968856]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground-truth Matrix:\\n\", Y_true[0])\n",
    "print(\"Completed Matrix:\\n\", Y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Matrix:\n",
      " [[-2.06063343e-01 -1.61200311e-01 -1.66148873e-02 -1.86683277e-02\n",
      "  -6.14295441e-02  1.35219428e-01 -3.58565121e-02  1.02945086e-01\n",
      "  -1.42249266e-02 -3.90438844e-02]\n",
      " [-5.51263397e-02 -4.16171175e-01 -3.82931602e-01  4.70745429e-02\n",
      "  -3.65278232e-02  6.90954716e-02 -6.41113080e-03 -3.14383579e-02\n",
      "  -2.02374296e-02 -1.10520122e-01]\n",
      " [-1.64963073e-01  4.50457518e-03 -3.56606813e-02 -6.89684301e-02\n",
      "  -6.88199135e-03  3.95933849e-02 -1.43023353e-02  1.15055551e-01\n",
      "   1.46816316e-01 -3.24425314e-02]\n",
      " [ 1.14287025e-01 -2.87580699e-02  1.54578998e-01 -7.58863601e-02\n",
      "  -2.00648876e-01 -2.87457677e-02 -4.39258798e-02 -4.07818883e-02\n",
      "  -1.33657024e-01 -5.80637183e-02]\n",
      " [ 7.83653369e-02  7.42226520e-02 -3.00327720e-01  1.12839470e-01\n",
      "   2.94844724e-02 -2.51716733e-02  5.58689833e-02  2.90776881e-02\n",
      "  -3.80557635e-02 -1.63232625e-02]\n",
      " [ 8.72469820e-02  2.76804905e-01 -1.35088919e-01  3.07743057e-01\n",
      "   1.77960289e-01  1.24674110e-02 -2.99586211e-02 -1.23807408e-01\n",
      "   8.61213323e-02 -1.47586497e-02]\n",
      " [ 1.28119436e-01 -1.04430620e-02 -1.29955027e-01 -5.01292518e-02\n",
      "  -1.14520473e-01 -9.17244285e-02  3.14717442e-02 -2.91617702e-02\n",
      "  -4.76707726e-02 -2.84996817e-02]\n",
      " [-3.78288760e-02 -2.21915820e-01 -1.32458290e-02  3.38871982e-02\n",
      "  -2.60597496e-01 -2.67090999e-02 -4.40953668e-03  1.44223349e-02\n",
      "  -5.32255881e-02 -1.12139893e-01]\n",
      " [-3.50517914e-02  1.02335849e-01 -2.92991939e-02  6.54532249e-02\n",
      "   3.30429501e-01  1.11793157e-01 -3.77484136e-02  1.42229275e-01\n",
      "  -3.27873225e-02  1.27147952e-01]\n",
      " [ 2.48519121e-01 -2.29740449e-02  8.16078762e-02  9.01993286e-02\n",
      "  -1.43609408e-02 -3.22840532e-02 -3.33937193e-02 -4.11418021e-02\n",
      "   8.82220732e-02  7.24200447e-02]\n",
      " [ 1.38425345e-01  1.13460810e-02 -2.80921140e-02  3.61408685e-01\n",
      "  -9.14834693e-02  2.94479153e-03  4.41477904e-02 -2.60319101e-02\n",
      "   9.47177697e-03  1.01357087e-01]\n",
      " [-1.28778402e-02 -2.20927734e-01  9.88686979e-02  7.84836244e-02\n",
      "  -2.28411963e-02 -2.22514272e-01  6.01776015e-03 -1.93270771e-02\n",
      "   3.80181727e-02 -3.54323666e-02]\n",
      " [-2.99865837e-02  7.83639988e-02 -1.71248541e-01  9.29515119e-02\n",
      "   2.62333500e-01 -2.02972384e-02  1.49470561e-02 -2.09938835e-02\n",
      "  -6.53676058e-02 -3.10869720e-02]\n",
      " [-4.96332251e-01 -3.18683900e-02 -2.73307813e-01 -5.88144139e-02\n",
      "   1.34800004e-01  9.43922787e-02  8.54013171e-02 -2.44216955e-02\n",
      "   1.24015561e-01  3.93999025e-02]\n",
      " [-4.51588832e-02 -4.75787802e-02 -2.18791689e-01 -2.79679502e-02\n",
      "   2.10718750e-02 -3.30202983e-02 -3.29630929e-02 -7.62649941e-02\n",
      "  -1.92668958e-01  1.11460386e-02]\n",
      " [ 1.37044961e-01 -8.29997824e-02 -1.84345394e-02 -2.02632606e-02\n",
      "  -1.73452747e-02 -5.21241485e-03 -2.43122157e-03 -1.03348538e-01\n",
      "  -3.69950764e-01  1.41863067e-01]\n",
      " [-3.40155902e-02 -3.13519552e-02  1.75339635e-01 -2.70710566e-02\n",
      "  -4.13435416e-01  1.01568357e-01  6.14774056e-02  4.09221392e-02\n",
      "  -3.16918208e-02 -2.04193536e-02]\n",
      " [ 1.44550457e-01 -7.30954818e-02  1.02018723e-01  9.95033054e-03\n",
      "   3.65827410e-02 -2.64942096e-02 -4.66651205e-02  8.55202236e-02\n",
      "  -4.69067784e-01 -1.76424850e-02]\n",
      " [-3.11966094e-02  3.32794243e-02 -2.31528204e-02 -2.30267996e-02\n",
      "   1.24233101e-01 -4.63623179e-02  1.25535812e-01  2.57632659e-02\n",
      "   4.82868039e-02 -2.05674745e-01]\n",
      " [-2.21446301e-01 -2.45181424e-02  2.36681471e-02  3.13784725e-02\n",
      "   6.41091223e-02 -6.83071269e-02 -3.04373735e-01  6.72129639e-02\n",
      "  -4.99323303e-03 -3.78473407e-02]\n",
      " [ 1.42127838e-01  5.52934320e-02 -3.35765713e-02 -1.01879937e-02\n",
      "  -2.85444626e-02 -2.04943098e-02  3.28221205e-01  1.03388451e-01\n",
      "  -3.11951820e-02 -4.11790075e-02]\n",
      " [-1.72340632e-02  6.19388892e-02 -9.08500619e-02 -3.09247145e-01\n",
      "  -2.42931602e-02  1.63057938e-01 -4.10428127e-01 -3.51592389e-02\n",
      "  -4.03261172e-02  1.37959387e-01]\n",
      " [-3.12057072e-02  6.27492028e-02 -9.20884348e-03 -2.27515340e-01\n",
      "  -3.81214027e-02 -3.60278246e-03 -2.94380874e-01 -1.07287824e-01\n",
      "  -1.21137783e-01  3.41105854e-02]\n",
      " [ 8.28792948e-02  4.64631433e-02  6.92158341e-02 -3.68010773e-02\n",
      "   9.70347592e-02  3.41575780e-02 -4.98523770e-01 -1.30745874e-02\n",
      "   3.11151553e-02 -1.99653317e-01]\n",
      " [-2.00209196e-01 -1.20967105e-01  6.43918803e-02 -2.05431374e-01\n",
      "   6.34815439e-02 -4.59111978e-01 -2.16051433e-02  1.47634792e-02\n",
      "  -4.84720992e-01  6.16270162e-02]\n",
      " [ 6.75012691e-03 -7.55796497e-02  1.36806295e-01  2.72521343e-03\n",
      "  -3.44334909e-02 -2.93298511e-02 -2.81790668e-02 -7.64413814e-02\n",
      "   1.78222061e-01 -2.92863936e-02]\n",
      " [-3.71903262e-02  1.30815548e-01 -2.97668031e-02 -2.75641939e-01\n",
      "   3.18343788e-01 -1.73486089e-01  1.53158598e-01  1.21147068e-01\n",
      "  -5.06836447e-02  6.27820017e-02]\n",
      " [-3.22268832e-02  6.45531819e-02 -8.96577938e-02 -9.24422343e-03\n",
      "  -2.35239694e-01 -3.20842231e-01  1.54118271e-01  7.45682219e-02\n",
      "   2.42020217e-03 -3.18343468e-02]\n",
      " [-4.56930815e-02  6.08117694e-02 -4.03847803e-01  1.37016720e-01\n",
      "   1.52930766e-01 -1.01319396e-01 -9.31306250e-02 -5.94233618e-02\n",
      "  -1.08747532e-01 -1.93899694e-01]\n",
      " [ 6.03782085e-02 -2.24093411e-02 -3.94205295e-02  1.58177622e-01\n",
      "  -2.90193165e-02 -2.66215673e-03 -2.47625574e-02 -4.68261749e-02\n",
      "  -5.87353624e-02  7.14777033e-02]\n",
      " [ 1.45245541e-01  6.04019746e-02  4.21115653e-02 -3.15284019e-02\n",
      "   1.04374373e-01  2.10136453e-02  5.43058134e-02  1.22964510e-01\n",
      "   1.50000736e-01 -8.53580900e-02]\n",
      " [-2.63888037e-02 -1.32635023e-02 -1.65697432e-01  3.45310661e-02\n",
      "  -4.14696742e-02 -1.15897790e-01 -2.70980053e-02 -3.07539541e-02\n",
      "  -4.15630644e-03  6.04348637e-02]\n",
      " [-1.29182713e-01 -2.80469255e-01 -4.24519001e-02 -4.03576452e-02\n",
      "  -1.01930646e-02 -4.00772426e-01 -3.74519522e-02 -2.93674979e-02\n",
      "   1.21670366e-01 -6.54020943e-02]\n",
      " [ 1.42939639e-01  1.26914727e-01  1.29078160e-01 -3.29887109e-02\n",
      "   5.76934230e-02 -5.73859769e-02  1.05841050e-01 -3.76120562e-02\n",
      "   7.99228554e-02 -7.17788457e-02]\n",
      " [-3.44010761e-01 -4.18662305e-02 -3.89596203e-01 -6.79625731e-02\n",
      "  -2.51987143e-01 -2.65966135e-02 -3.78908489e-02 -3.34645943e-02\n",
      "   8.19273905e-03  1.50411649e-01]\n",
      " [ 1.40265109e-01  6.58506147e-02  7.45222247e-02 -1.47230775e-02\n",
      "  -2.69725503e-02 -6.65278473e-02  1.08658418e-01  1.41382438e-01\n",
      "   1.10883868e-01 -4.98018398e-02]\n",
      " [-3.03726020e-02 -1.28968825e-01 -2.00463185e-01 -2.70216022e-02\n",
      "   1.09202741e-01 -3.65613222e-01 -1.36291461e-02 -2.23175553e-02\n",
      "  -1.00713924e-01 -1.80322742e-02]\n",
      " [ 2.52042118e-02  1.64567087e-01 -2.88515314e-01  1.37457353e-01\n",
      "   1.45273594e-02  1.42057576e-01  1.65283433e-04 -2.19626103e-01\n",
      "  -3.77826742e-02  1.37963120e-02]\n",
      " [-5.49977080e-01  3.12149475e-01  3.16072269e-01 -4.70254607e-01\n",
      "  -3.18933084e-02 -2.23632394e-01 -3.47535624e-02 -3.61526372e-02\n",
      "  -8.57137071e-03 -2.81503701e-01]\n",
      " [ 2.36635938e-02 -3.31256409e-02 -1.34088409e-01  1.50062631e-01\n",
      "  -9.60044510e-02 -1.98788327e-01 -5.60397013e-02 -1.53444561e-01\n",
      "  -2.50884167e-01  3.70666032e-01]\n",
      " [-4.02941793e-01  2.56189749e-02 -6.85038812e-02 -1.16139389e-01\n",
      "   3.33801717e-01  1.78170115e-02 -3.81536016e-02 -9.38381118e-02\n",
      "  -1.70406351e-02 -1.37488525e-01]\n",
      " [ 6.33342726e-04 -2.56178666e-01  2.86203122e-02 -4.79025350e-02\n",
      "  -4.55594680e-01 -3.16679778e-02  4.92190155e-02 -3.05883047e-02\n",
      "  -3.37423582e-02 -1.75015222e-02]\n",
      " [ 1.40804409e-01  5.41160831e-02  6.29262447e-02  6.65691672e-02\n",
      "   3.77290470e-02  1.17653385e-01 -3.26462354e-01 -1.96447464e-02\n",
      "  -5.40883423e-02  1.11441616e-01]\n",
      " [ 1.54700225e-01  1.24154087e-01  1.35218403e-01 -6.96859733e-02\n",
      "   5.21429196e-02 -3.44073290e-02 -7.09473575e-02 -2.26325364e-01\n",
      "  -1.96327444e-02  1.48999276e-01]\n",
      " [ 5.91521422e-02 -1.92629807e-01 -8.23989454e-02 -6.00076263e-02\n",
      "  -1.90571401e-02 -6.61974382e-02  1.10596273e-01 -3.82248253e-02\n",
      "  -3.57865526e-02 -7.09001687e-02]\n",
      " [ 4.32089559e-02  1.38700186e-01  5.91330504e-02 -2.60935382e-02\n",
      "   2.77564902e-02 -3.31026884e-01  1.52481510e-01  3.33304014e-01\n",
      "  -2.31491678e-01 -5.44901132e-01]\n",
      " [-7.93700577e-03  6.80815024e-02  1.09825574e-01 -9.90184544e-02\n",
      "  -1.76940938e-01  1.47972235e-01 -2.97401985e-02 -4.27805322e-02\n",
      "  -6.54288005e-02 -2.82491355e-02]\n",
      " [-6.01470830e-02 -8.14801795e-03 -3.22166069e-01 -9.65469539e-02\n",
      "   1.38509183e-01  1.42264529e-01 -6.79607518e-02 -2.31952495e-01\n",
      "   9.54180181e-02 -8.33784515e-03]\n",
      " [ 1.36545463e-01 -3.00098270e-02  3.16596894e-01 -3.22650746e-02\n",
      "  -3.71773665e-02 -1.93971321e-01  5.71578496e-02  3.86457147e-02\n",
      "  -9.41739185e-03 -5.70323948e-02]\n",
      " [ 6.73151276e-02 -3.48476357e-01 -3.75311095e-02 -2.34923844e-01\n",
      "  -6.44450505e-02 -3.25381719e-02 -6.86915486e-03  1.06309602e-01\n",
      "  -2.18860138e-02  1.46712467e-01]]\n",
      "Ground Truth Matrix:\n",
      " [[ 3.82788060e-01  9.40006837e-01  3.00019263e+00  1.04005497e+00\n",
      "   6.97661142e-01  6.54051605e-01 -2.29678630e-01 -8.66815902e-01\n",
      "   1.22421269e-01 -5.18203450e-01]\n",
      " [-5.21627768e-01 -1.28095340e+00 -4.08838192e+00 -1.41728965e+00\n",
      "  -9.50707356e-01 -8.91280358e-01  3.12984557e-01  1.18121564e+00\n",
      "  -1.66824256e-01  7.06159197e-01]\n",
      " [ 1.39113965e-01  3.41620057e-01  1.09033885e+00  3.77979845e-01\n",
      "   2.53546069e-01  2.37697363e-01 -8.34704851e-02 -3.15020791e-01\n",
      "   4.44906986e-02 -1.88327026e-01]\n",
      " [ 2.13258325e-01  5.23695236e-01  1.67146293e+00  5.79433907e-01\n",
      "   3.88679957e-01  3.64384275e-01 -1.27958223e-01 -4.82919209e-01\n",
      "   6.82031586e-02 -2.88700749e-01]\n",
      " [-5.09684934e-01 -1.25162557e+00 -3.99477711e+00 -1.38484035e+00\n",
      "  -9.28940607e-01 -8.70874209e-01  3.05818675e-01  1.15417133e+00\n",
      "  -1.63004761e-01  6.89991457e-01]\n",
      " [ 1.95863267e-01  4.80978456e-01  1.53512502e+00  5.32170634e-01\n",
      "   3.56976104e-01  3.34662173e-01 -1.17520925e-01 -4.43528448e-01\n",
      "   6.26399623e-02 -2.65152002e-01]\n",
      " [-3.94144491e-01 -9.67894653e-01 -3.08920136e+00 -1.07091098e+00\n",
      "  -7.18359125e-01 -6.73455794e-01  2.36492661e-01  8.92532313e-01\n",
      "  -1.26053223e-01  5.33577341e-01]\n",
      " [-7.21522358e-01 -1.77183152e+00 -5.65510340e+00 -1.96041359e+00\n",
      "  -1.31503086e+00 -1.23283066e+00  4.32924337e-01  1.63387294e+00\n",
      "  -2.30753495e-01  9.76768646e-01]\n",
      " [-3.28165748e-01 -8.05871653e-01 -2.57207724e+00 -8.91643323e-01\n",
      "  -5.98107711e-01 -5.60721079e-01  1.96904416e-01  7.43124769e-01\n",
      "  -1.04952248e-01  4.44257908e-01]\n",
      " [-2.19363416e-01 -5.38687416e-01 -1.71931304e+00 -5.96021756e-01\n",
      "  -3.99806962e-01 -3.74815751e-01  1.31621370e-01  4.96744066e-01\n",
      "  -7.01556568e-02  2.96965582e-01]\n",
      " [ 4.47823097e-01  1.09971239e+00  3.50992023e+00  1.21675853e+00\n",
      "   8.16192578e-01  7.65173855e-01 -2.68700637e-01 -1.01408644e+00\n",
      "   1.43220433e-01 -6.06245330e-01]\n",
      " [ 3.31587529e-01  8.14274468e-01  2.59889626e+00  9.00940479e-01\n",
      "   6.04344174e-01  5.66567712e-01 -1.98957536e-01 -7.50873323e-01\n",
      "   1.06046584e-01 -4.48890180e-01]\n",
      " [-5.95578136e-01 -1.46255220e+00 -4.66798555e+00 -1.61821662e+00\n",
      "  -1.08548768e+00 -1.01763581e+00  3.57355897e-01  1.34867477e+00\n",
      "  -1.90474675e-01  8.06270303e-01]\n",
      " [-9.48294598e-01 -2.32871267e+00 -7.43248486e+00 -2.57656551e+00\n",
      "  -1.72834098e+00 -1.62030551e+00  5.68991114e-01  2.14739414e+00\n",
      "  -3.03278603e-01  1.28376400e+00]\n",
      " [ 1.42634866e-01  3.50266279e-01  1.11793475e+00  3.87546315e-01\n",
      "   2.59963186e-01  2.43713357e-01 -8.55830788e-02 -3.22993800e-01\n",
      "   4.56167345e-02 -1.93093483e-01]\n",
      " [ 3.21035992e-01  7.88363221e-01  2.51619608e+00  8.72271409e-01\n",
      "   5.85113175e-01  5.48538808e-01 -1.92626455e-01 -7.26979581e-01\n",
      "   1.02672047e-01 -4.34605925e-01]\n",
      " [-6.03333501e-01 -1.48159693e+00 -4.72877007e+00 -1.63928835e+00\n",
      "  -1.09962244e+00 -1.03088703e+00  3.62009233e-01  1.36623664e+00\n",
      "  -1.92954955e-01  8.16769211e-01]\n",
      " [ 2.37850213e-01  5.84085164e-01  1.86420772e+00  6.46251341e-01\n",
      "   4.33500594e-01  4.06403255e-01 -1.42713728e-01 -5.38607048e-01\n",
      "   7.60680072e-02 -3.21992282e-01]\n",
      " [ 3.11425671e-01  7.64763300e-01  2.44087290e+00  8.46159668e-01\n",
      "   5.67597613e-01  5.32118112e-01 -1.86860117e-01 -7.05217200e-01\n",
      "   9.95985241e-02 -4.21595849e-01]\n",
      " [-4.28220019e-01 -1.05157341e+00 -3.35627643e+00 -1.16349596e+00\n",
      "  -7.80464436e-01 -7.31679015e-01  2.56938493e-01  9.69695664e-01\n",
      "  -1.36951080e-01  5.79707453e-01]\n",
      " [ 7.78024820e-02  1.91058376e-01  6.09795491e-01  2.11393371e-01\n",
      "   1.41801101e-01  1.32937371e-01 -4.66826669e-02 -1.76182164e-01\n",
      "   2.48823816e-02 -1.05325946e-01]\n",
      " [-2.14896199e-01 -5.27717340e-01 -1.68430017e+00 -5.83884097e-01\n",
      "  -3.91665111e-01 -3.67182833e-01  1.28940973e-01  4.86628144e-01\n",
      "  -6.87269750e-02  2.90918039e-01]\n",
      " [ 5.68194757e-02  1.39530725e-01  4.45336180e-01  1.54381457e-01\n",
      "   1.03557933e-01  9.70847140e-02 -3.40925455e-02 -1.28666566e-01\n",
      "   1.81717066e-02 -7.69199759e-02]\n",
      " [ 2.44026511e-01  5.99252204e-01  1.91261592e+00  6.63032661e-01\n",
      "   4.44757379e-01  4.16956399e-01 -1.46419600e-01 -5.52593150e-01\n",
      "   7.80432783e-02 -3.30353511e-01]\n",
      " [-1.30116147e-01 -3.19524251e-01 -1.01981631e+00 -3.53532308e-01\n",
      "  -2.37146843e-01 -2.22323223e-01  7.80716581e-02  2.94645412e-01\n",
      "  -4.16130636e-02  1.76146133e-01]\n",
      " [ 6.42047717e-03  1.57666685e-02  5.03220196e-02  1.74447688e-02\n",
      "   1.17018212e-02  1.09703615e-02 -3.85238349e-03 -1.45390421e-02\n",
      "   2.05336332e-03 -8.69178997e-03]\n",
      " [-2.54666373e-01 -6.25380353e-01 -1.99600838e+00 -6.91941718e-01\n",
      "  -4.64149359e-01 -4.35136222e-01  1.52803679e-01  5.76686906e-01\n",
      "  -8.14460633e-02  3.44757340e-01]\n",
      " [ 1.36634925e-01  3.35532314e-01  1.07090878e+00  3.71244163e-01\n",
      "   2.49027824e-01  2.33461545e-01 -8.19830231e-02 -3.09407053e-01\n",
      "   4.36978648e-02 -1.84970998e-01]\n",
      " [-1.42681923e-01 -3.50381836e-01 -1.11830357e+00 -3.87674171e-01\n",
      "  -2.60048951e-01 -2.43793761e-01  8.56113137e-02  3.23100360e-01\n",
      "  -4.56317840e-02  1.93157187e-01]\n",
      " [-3.91560047e-01 -9.61548072e-01 -3.06894516e+00 -1.06388891e+00\n",
      "  -7.13648773e-01 -6.69039878e-01  2.34941955e-01  8.86679891e-01\n",
      "  -1.25226680e-01  5.30078621e-01]\n",
      " [ 3.89914346e-01  9.57506749e-01  3.05604659e+00  1.05941746e+00\n",
      "   7.10649355e-01  6.66227948e-01 -2.33954510e-01 -8.82953234e-01\n",
      "   1.24700360e-01 -5.27850736e-01]\n",
      " [ 2.30177803e-01  5.65244143e-01  1.80407338e+00  6.25405006e-01\n",
      "   4.19517028e-01  3.93293777e-01 -1.38110166e-01 -5.21233030e-01\n",
      "   7.36142574e-02 -3.11605675e-01]\n",
      " [-3.88372342e-01 -9.53720074e-01 -3.04396077e+00 -1.05522776e+00\n",
      "  -7.07838932e-01 -6.63593200e-01  2.33029284e-01  8.79461397e-01\n",
      "  -1.24207205e-01  5.25763232e-01]\n",
      " [ 6.44879569e-01  1.58362098e+00  5.05439728e+00  1.75217117e+00\n",
      "   1.17534339e+00  1.10187480e+00 -3.86937503e-01 -1.46031688e+00\n",
      "   2.06242000e-01 -8.73012648e-01]\n",
      " [ 1.51274821e-01  3.71483284e-01  1.18565245e+00  4.11021518e-01\n",
      "   2.75710178e-01  2.58476033e-01 -9.07671822e-02 -3.42558804e-01\n",
      "   4.83799194e-02 -2.04789914e-01]\n",
      " [-2.10817998e-01 -5.17702562e-01 -1.65233629e+00 -5.72803413e-01\n",
      "  -3.84232270e-01 -3.60214605e-01  1.26493990e-01  4.77393138e-01\n",
      "  -6.74227060e-02  2.85397130e-01]\n",
      " [-7.25906128e-02 -1.78259668e-01 -5.68946224e-01 -1.97232452e-01\n",
      "  -1.32302062e-01 -1.24032100e-01  4.35554665e-02  1.64379990e-01\n",
      "  -2.32155489e-02  9.82703222e-02]\n",
      " [-3.55915250e-01 -8.74015685e-01 -2.78957057e+00 -9.67040156e-01\n",
      "  -6.48683346e-01 -6.08135322e-01  2.13554537e-01  8.05962961e-01\n",
      "  -1.13826948e-01  4.81824094e-01]\n",
      " [ 4.15573037e-01  1.02051641e+00  3.25715270e+00  1.12913345e+00\n",
      "   7.57414324e-01  7.10069723e-01 -2.49350113e-01 -9.41056827e-01\n",
      "   1.32906388e-01 -5.62586465e-01]\n",
      " [ 4.31320021e-01  1.05918604e+00  3.38057344e+00  1.17191882e+00\n",
      "   7.86114432e-01  7.36975840e-01 -2.58798542e-01 -9.76715557e-01\n",
      "   1.37942506e-01 -5.83904114e-01]\n",
      " [-1.78684227e-01 -4.38792149e-01 -1.40048021e+00 -4.85494295e-01\n",
      "  -3.25665963e-01 -3.05309172e-01  1.07213241e-01  4.04626857e-01\n",
      "  -5.71458521e-02  2.41895693e-01]\n",
      " [ 6.44298614e-02  1.58219434e-01  5.04984390e-01  1.75059268e-01\n",
      "   1.17428456e-01  1.10088215e-01 -3.86588921e-02 -1.45900132e-01\n",
      "   2.06056202e-02 -8.72226173e-02]\n",
      " [ 8.06416943e-01  1.98030586e+00  6.32048493e+00  2.19107658e+00\n",
      "   1.46975787e+00  1.37788597e+00 -4.83862373e-01 -1.82611503e+00\n",
      "   2.57904036e-01 -1.09169560e+00]\n",
      " [ 1.62906518e-01  4.00047067e-01  1.27681865e+00  4.42625441e-01\n",
      "   2.96909855e-01  2.78350556e-01 -9.77463766e-02 -3.68898550e-01\n",
      "   5.20999078e-02 -2.20536449e-01]\n",
      " [-2.08604990e-02 -5.12268112e-02 -1.63499131e-01 -5.66790556e-02\n",
      "  -3.80198890e-02 -3.56433345e-02  1.25166152e-02  4.72381826e-02\n",
      "  -6.67149534e-03  2.82401247e-02]\n",
      " [ 2.55518259e-01  6.27472316e-01  2.00268524e+00  6.94256335e-01\n",
      "   4.65701987e-01  4.36591798e-01 -1.53314823e-01 -5.78615984e-01\n",
      "   8.17185089e-02 -3.45910589e-01]\n",
      " [-3.97268886e-01 -9.75567183e-01 -3.11368955e+00 -1.07940013e+00\n",
      "  -7.24053579e-01 -6.78794299e-01  2.38367345e-01  8.99607444e-01\n",
      "  -1.27052451e-01  5.37807024e-01]\n",
      " [ 4.62177686e-02  1.13496274e-01  3.62242774e-01  1.25576070e-01\n",
      "   8.42354937e-02  7.89700853e-02 -2.77313608e-02 -1.04659212e-01\n",
      "   1.47811243e-02 -6.25678010e-02]\n",
      " [ 3.93875562e-01  9.67234248e-01  3.08709357e+00  1.07018029e+00\n",
      "   7.17868981e-01  6.72996288e-01 -2.36331299e-01 -8.91923329e-01\n",
      "   1.25967216e-01 -5.33213275e-01]\n",
      " [-2.03396100e-01 -4.99476720e-01 -1.59416540e+00 -5.52637732e-01\n",
      "  -3.70705281e-01 -3.47533164e-01  1.22040739e-01  4.60586399e-01\n",
      "  -6.50490735e-02  2.75349656e-01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def forward_pass(x, weights, activations):\n",
    "    z = x\n",
    "    for w, act in zip(weights, activations):\n",
    "        z = act(np.dot(w, z))  # Apply weight matrix and activation function\n",
    "    return z\n",
    "\n",
    "def neural_tangent_kernel(X, weights, activations):\n",
    "    n = X.shape[1]  # Number of samples\n",
    "    K = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            x_i = X[:, i].reshape(-1, 1)  # Reshape to (2, 1) for input to the first layer\n",
    "            x_j = X[:, j].reshape(-1, 1)  # Reshape to (2, 1)\n",
    "            z_i = forward_pass(x_i, weights, activations)\n",
    "            z_j = forward_pass(x_j, weights, activations)\n",
    "            K[i, j] = np.dot(z_i.T, z_j).item()  # Compute dot product between z_i and z_j\n",
    "    return K\n",
    "\n",
    "def kernel_ridge_regression(K, Y, S, alpha=1e-3):\n",
    "    m, n = Y.shape\n",
    "    observed_indices = np.where(S.ravel() == 1)[0]  # Observed indices\n",
    "    y_obs = Y.ravel()[observed_indices]\n",
    "    K_obs = K[np.ix_(observed_indices, observed_indices)]\n",
    "    \n",
    "    alpha_I = alpha * np.eye(len(observed_indices))\n",
    "    alphas = np.linalg.solve(K_obs + alpha_I, y_obs)\n",
    "    \n",
    "    K_obs_full = K[:, observed_indices]\n",
    "    Y_pred = K_obs_full @ alphas\n",
    "    return Y_pred.reshape(m, n)\n",
    "\n",
    "# Example usage\n",
    "m, n = 50, 10\n",
    "rank = 1\n",
    "sparsity = 0\n",
    "X = np.random.rand(2, m * n)  # Inputs as (2, m*n)\n",
    "weights = [np.random.randn(10, 2)] + [np.random.randn(10, 10) for _ in range(9)]  # 10 layers\n",
    "activations = [relu] * 10  # 10 layers with ReLU activations\n",
    "\n",
    "# Assuming generate_synthetic_data is defined elsewhere\n",
    "Y, S, Y_true = generate_synthetic_data(m, n, rank, sparsity)\n",
    "\n",
    "K = neural_tangent_kernel(X, weights, activations)\n",
    "Y_pred = kernel_ridge_regression(K, Y, S)\n",
    "\n",
    "print(\"Completed Matrix:\\n\", Y_pred)\n",
    "print(\"Ground Truth Matrix:\\n\", Y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Matrix:\n",
      " [-0.20606334 -0.05512634 -0.16496307  0.11428703  0.07836534  0.08724698\n",
      "  0.12811944 -0.03782888 -0.03505179  0.24851912  0.13842535 -0.01287784\n",
      " -0.02998658 -0.49633225 -0.04515888  0.13704496 -0.03401559  0.14455046\n",
      " -0.03119661 -0.2214463   0.14212784 -0.01723406 -0.03120571  0.08287929\n",
      " -0.2002092   0.00675013 -0.03719033 -0.03222688 -0.04569308  0.06037821\n",
      "  0.14524554 -0.0263888  -0.12918271  0.14293964 -0.34401076  0.14026511\n",
      " -0.0303726   0.02520421 -0.54997708  0.02366359 -0.40294179  0.00063334\n",
      "  0.14080441  0.15470022  0.05915214  0.04320896 -0.00793701 -0.06014708\n",
      "  0.13654546  0.06731513]\n",
      "Ground Truth Matrix:\n",
      " [ 0.38278806 -0.52162777  0.13911397  0.21325832 -0.50968493  0.19586327\n",
      " -0.39414449 -0.72152236 -0.32816575 -0.21936342  0.4478231   0.33158753\n",
      " -0.59557814 -0.9482946   0.14263487  0.32103599 -0.6033335   0.23785021\n",
      "  0.31142567 -0.42822002  0.07780248 -0.2148962   0.05681948  0.24402651\n",
      " -0.13011615  0.00642048 -0.25466637  0.13663492 -0.14268192 -0.39156005\n",
      "  0.38991435  0.2301778  -0.38837234  0.64487957  0.15127482 -0.210818\n",
      " -0.07259061 -0.35591525  0.41557304  0.43132002 -0.17868423  0.06442986\n",
      "  0.80641694  0.16290652 -0.0208605   0.25551826 -0.39726889  0.04621777\n",
      "  0.39387556 -0.2033961 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Completed Matrix:\\n\", Y_pred.T[0])\n",
    "print(\"Ground Truth Matrix:\\n\", Y_true.T[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
